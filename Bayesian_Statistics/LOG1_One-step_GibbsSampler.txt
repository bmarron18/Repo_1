Title: LOG16 -- Example p. 61 Hoff
Date: 12/24/14
Author: Marron



GOAL: Understanding the essence of the MCMC algorithm: one-step Gibbs sampling to get the posterior predicitive;
      Duplication of Hoff example, p. 61; compare R and BUGS

#### general problem statement and background ################################################
	*using one-step Gibbs sampler (MCMC) to generate the posterior predicitve distribution in R and BUGS
	 from a (one-parameter) Poisson sampling distribution model
	*evaluations:
		1. MCMC diagnostics --- (see codamenu()!!) 
			(a) autocorrelation
				(i) autocorr(x, lags = c(0, 1, 5, 10, 50), relative=TRUE)	#pkg 'coda'; x = mcmc object
				(ii) acfplot(x, type-"l")					#pkg 'coda'
				(iii) acf(x)							#pkg 'stats'
				(iv) effectiveSize(x)					#pkg 'coda'
											#compare to a Markov chain w/ no autocorrelation 

			(b) convergence
				(i)  plot(1:# iterations, x, type="l")			#ideally, the "fuzzy catepillar"
				(ii) heidel.diag(x, pvalue=0.05)	 			#uses the Cramer-von-Mises statistic to test the 
												#null hypothesis that the sampled values come from 
												#a stationary distribution; pkg "coda"
				(iii) geweke.diag(x, frac1=0.1, frac2=0.5)		#two sample means collected from different parts of 
											#the chain are assumed equal if MC convergence; test 
											#statistic is a standard Z-score (difference/se); pkg "coda"
											#H0: μ1 = μ2	==> μ1 - μ2 = 0
											#H1: μ1 ≠  μ2

				(iv)gelman.diag(x, confidence = 0.95) 		#The ‘potential scale reduction factor’ is calculated for each variable 
										#in x, together with upper and lower confidence limits. Approximate 
										#convergence is diagnosed when the upper limit is close to 1; pkg "coda"
										#only convergence diagnostic in BUGS (p.75 BUGS Book)





		2. model goodness-of-fit test
			(a) stat1 -- the reciprocal (inverse) coefficient of variation (CV) [=(mean/sd)] for datasets generated by
	   		    the posterior predictive distribution and for the original data set. 
				* Compare by locating test.stat1 for the original dataset (a single value) on the histogram of 
				  the test.stat1 values for calculated for the datasets generated by the posterior predictive distribution
				* the signal-to-noise ratio (in signal processing)
				* Hoff's test stat (p. 232)








#### datasets ########################################################################
http://www.stat.washington.edu/hoff/Book/Data/hwdata/
source("http://www.stat.washington.edu/hoff/Book/Data/data/chapter7.r")		#will open directly from the website
women's data not avalable! Use HW data 

menchild30bach.dat		
menchild30nobach.dat	



#### problem sketch #########################################################
	*one-parameter Poisson model of number of children 
		- men over 30 w/ bachelor's degree
		- men over 30 w/o bachelor's degree
	*model with R and BUGS




##########################################################################################################
################ One-step Gibbs Sampler (MCMC) in R ######################################################
##########################################################################################################


#### Gibbs sampler algorithm for R #################################################

p(θ|data) <== p(data|θ) x p(θ) 					#the posterior from (likelihood x prior)

		p(data|θ):
		p(Y.99.d|Z1) ~ dpois(Z1)			#the model: Y.99.d ~ Poisson (θ1)
		p(Y.98.d|Z2) ~ dpois(Z2)			#the model: Y.98.d ~ Poisson (θ2)

		p(θ):
		p(Z1) ~ dgamma(W1, W2)
			W1 = 2
			W2 = 1 
		p(Z2) ~ dgamma(V1, V2)
			V1 = 2
			V2 = 1 

p(θ|data):
p(Z1|Y.99.d) ~ dgamma(W1+Y.99.d.t, W2+Y.99.d.c)		#the (analytical) posterior b/c conjugate prior available for Poisson
p(Z2|Y.98.d) ~ dgamma(V1+Y.98.d.t, V2+Y.98.d.c)		#the (analytical) posterior b/c conjugate prior available for Poisson




p(pred.data|data)  <== ∫p(pred.data|θ) x p(θ|data)	#the posterior predicitive from integration [(presumed likelihood) x (posterior)]

		   <==	1. pull set of random samples (ie Monte Carlo) from p(θ|data)		#the posterior
			2. plug set of random samples from 1. into p(pred.data|θ)		#the presumed likelihood
			3. pull set of random samples (ie Monte Carlo) from p(pred.data|θ)	#get a set of pred.data from the posterior predictive


p(pred.data|data):
p(Y.99.pred|Y.99.d) <==	 ∫dpois(Z1) x dgamma(W1+Y.99.d.t, W2+Y.99.d.c)

		    <==	1. pull set of 10000 random samples of Z1 from dgamma (W1+Y.99.d.t, W2+Y.99.d.c)
			2. plug set of 10000 random samples of Z1 into dpois(Z1)	
			3. pull set of 10000 random samples of Y.99.pred from dpois(Z1)				#ONE value y.99.pred per value Z1


p(pred.data|data):
p(Y.98.pred|Y.98.d) <==	 ∫dpois(Z2) x dgamma(V1+Y.98.d.t, V2+Y.98.d.c)	

		    <==	1. pull set of 10000 random samples of Z2 from dgamma(V1+Y.98.d.t, V2+Y.98.d.c)
			2. plug set of 10000 random samples of Z2 into dpois(Z2)
			3. pull set of 10000 random samples of Y.98.pred from dpois(Z2)



#### data prep1 in R #################################################################
menchild30bach <- as.data.frame(scan("~/Desktop/menchild30bach.dat", nlines=2)		#scan()!!!
length(menchild30bach[,1])
[1] 58
sum(menchild30bach[,1])
[1] 54
mean(menchild30bach[,1])
[1] 0.9310345



menchild30nobach <- as.data.frame(scan("~/Desktop/menchild30nobach.dat", nlines=6))
length(menchild30nobach[,1])
[1] 218
sum(menchild30nobach[,1])
[1] 305
mean(menchild30nobach[,1])
[1] 1.399083



#### the R model (One-step Gibbs sampling) #################################################################
set.seed(47)
W1 <- V1 <- 2
W2 <- V2 <- 1

------
Y.99.d.c <- length(menchild30bach[,1])			#count of Y.99.d
Y.99.d.t <- sum(menchild30bach[,1])			#total of Y.99.d
Z1.mc <- rgamma(10000, W1+Y.99.d.t, W2+Y.99.d.c)
Y.99.pred <- rpois(10000, Z1.mc)			#pulls a single-valued dataset from the Poisson dist generated by
							#each of the Z1.mc (in sequence); simulates 10000, single-valued datasets from 
							#Y.99.pred (= posterior predictive dist); datasets saved


Y.98.d.c <- length(menchild30nobach[,1])
Y.98.d.t <- sum(menchild30nobach[,1])
Z2.mc <- rgamma(10000, V1+Y.98.d.t, V2+Y.98.d.c)
Y.98.pred <- rpois(10000, Z2.mc)

------
Y.99.mean.pred.d<-NULL
Y.99.stat1.pred.d<-NULL
for (i in 1:10000){

	Z1 <- rgamma(1, W1+Y.99.d.t, W2+Y.99.d.c)
	Y.99.pred.d <- rpois(Y.99.d.c, Z1)			#pulls a 58-valued dataset from the Poisson dist generated by
								#each (single) Z1; simulates 10000, 58-valued datasets from Y.99.pred;
								#datasets not saved

	Y.99.mean.pred.d <- c(					#collect the mean from each dataset generated and save
			Y.99.mean.pred.d,
			mean(Y.99.pred.d)
			)

	Y.99.stat1.pred.d <- c(					#collect stat1 from each dataset generated and save
			Y.99.stat1.pred.d, 
			mean(Y.99.pred.d)/sd(Y.99.pred.d)	#stat1 = reciprocal CV (aka signal-to-noise ratio) 
			)

	} 


Y.98.mean.pred.d<-NULL
Y.98.stat1.pred.d<-NULL
for (i in 1:10000){

	
	Z2 <- rgamma(1, V1+Y.98.d.t, V2+Y.98.d.c)
	Y.98.pred.d <- rpois(Y.98.d.c, Z2)			
								

	Y.98.mean.pred.d <- c(					
			Y.98.mean.pred.d,
			mean(Y.98.pred.d)
			)

	Y.98.stat1.pred.d <- c(					
			Y.98.stat1.pred.d, 
			mean(Y.98.pred.d)/sd(Y.98.pred.d)
			)

	} 



-------------------- outputs ------------------------------
mean(Z1.mc)
[1] 0.9492558
mean(Z2.mc)
[1] 1.402185



mean(Y.99.pred)
[1] 0.9509
mean(Y.99.mean.pred.d)
[1] 0.9500707
mean(Y.99.stat1.pred.d)
[1] 0.982022



mean(Y.98.pred)
[1] 1.4032
mean(Y.98.mean.pred.d)
[1] 1.402433
mean(Y.98.stat1.pred.d)
[1] 1.18756




mean(Y.99.pred>Y.98.pred)		#predictive prob that pop Y.99 has more children than pop Y.98
[1] 0.2555
mean(Y.99.pred<Y.98.pred)		#predictive prob that pop Y.98 has more children than pop Y.99
[1] 0.4696
mean(Y.99.pred - Y.98.pred)		#mean difference in the number of children between Y.99 and Y.98
[1] -0.4523
mean(Y.98.pred - Y.99.pred)
[1] 0.4523


--------- evaluations ---------------------------------------------
==== 1. MCMC diagnostics ====
	(a) autocorrelation
		(i) autocorr(x, lags = c(0, 1, 5, 10, 50), relative=TRUE)	#pkg 'coda'; x = mcmc object

Z1.mc.object<-as.mcmc(Z1.mc)
autocorr(Z1.mc.object, lags = c(0, 1, 5, 10, 50), relative=TRUE)
                [,1]
Lag 0   1.0000000000	
Lag 1   0.0173810217
Lag 5   0.0195989503
Lag 10 -0.0007133851
Lag 50 -0.0159407560

Z2.mc.object<-as.mcmc(Z2.mc)
autocorr(Z2.mc.object, lags = c(0, 1, 5, 10, 50), relative=TRUE)
               [,1]
Lag 0   1.000000000
Lag 1   0.016119721
Lag 5   0.013511216
Lag 10 -0.007693735
Lag 50 -0.010302739



		(ii) acf(x)							#pkg 'stats'
acf(Z1.mc)									#gives a plot
acf(Z2.mc)

		(iii) acfplot(x, type-"l")					#pkg 'coda'
acfplot(Z1.mc.object)								#gives a plot
acfplot(Z2.mc.object)


		(iv) effectiveSize(x)					#pkg 'coda'
									#compare to a Markov chain w/ no autocorrelation
effectiveSize(Z1.mc.object)
    var1 
9632.858

effectiveSize(Z2.mc.object)
    var1 
9681.752 


	(b) convergence
		(i)  plot(1:(# iterations), x, type="l")		#ideally, the "fuzzy catepillar"
plot(1:10000, Z1.mc, type="l")
plot(1:10000, Z2.mc, type="l")


		(ii) heidel.diag(x, pvalue=0.05)
heidel.diag(Z1.mc.object, pvalue=0.05)
    Stationarity start     p-value
     test         iteration        
var1 passed       1         0.0624 
                              
     Halfwidth Mean  Halfwidth
     test                     
var1 passed    0.949 0.00251  



heidel.diag(Z2.mc.object, pvalue=0.05)
     Stationarity start     p-value
     test         iteration        
var1 passed       1         0.135  
                             
     Halfwidth Mean Halfwidth
     test                    
var1 passed    1.4  0.00158  


		(iii) geweke.diag(x, frac1=0.1, frac2=0.5)
geweke.diag(Z1.mc.object, frac1=0.1, frac2=0.5)
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

   var1 
-0.4471 


geweke.diag(Z2.mc.object, frac1=0.1, frac2=0.5)
Fraction in 1st window = 0.1
Fraction in 2nd window = 0.5 

  var1 
-0.265 

		(iv) gelman.diag(x, confidence = 0.95)

set.seed(74)
Z1b.mc <- rgamma(10000, W1+Y.99.d.t, W2+Y.99.d.c)		#generate another chain starting from a new seed
Z1b.mc.object <- as.mcmc(Z1b.mc)				#transform to mcmc object

Z1.mcmc.list <- mcmc.list(Z1.mc.object, Z1b.mc.object)		#create a mcmc.list; needed for gelman.diag()
gelman.diag(Z1.mcmc.list, confidence = 0.95, multivariate=FALSE)

Potential scale reduction factors:

     Point est. Upper C.I.
[1,]          1          1




==== 2. model goodness-of-fit test ====
	(a) stat1
------------------------------ Y.99.d vs Y.99.d ~ Poisson(θ) --------------------
Y.99.d<-menchild30bach[,1]
mean(Y.99.d)/sd(Y.99.d)							#stat1 for Y.99.d
[1] 0.8541447
hist(Y.99.stat1.pred.d)							#general look to see where the sta1 of the data falls in comparison to the 
									#stat1 of the 10000 simulated datasets; looks OK

Y.99.stat1.coda.pred.d<-as.mcmc(Y.99.stat1.pred.d)			#read R output into CODA (as mcmc object) and ¡¡¡use codamenu()!!!!
codamenu()
CODA startup menu 

1: Read BUGS output files
2: Use an mcmc object
3: Quit

Selection: 2

Enter name of saved object (or type "exit" to quit) 
1:
Y.99.stat1.coda.pred.d
Checking effective sample size ...OK
CODA Main Menu 

1: Output Analysis
2: Diagnostics
3: List/Change Options
4: Quit

Selection: 1

CODA Output Analysis menu 

1: Plots
2: Statistics
3: List/Change Options
4: Return to Main Menu

Selection: 1						#gives	a) ideally, the "fuzzy catepillar"; plot((# iterations), x, type="l") 
								b) a smoothed histogram of the mcmc object (data); densityplot()



densityplot(Y.99.stat1.coda.pred.d, 								#'coda' plot; a trellis ('lattice') object
	    xlab ="stat1 (inverse CV)"
	)

trellis.last.object(panel = function(...) {							#'lattice' command
			panel.abline(v = 0.8541447, lty = "solid", col = "black")		#add a vert line
            		panel.densityplot(...)
        }
)												#Model appears very reasonable


CODA Output Analysis menu 

1: Plots
2: Statistics
3: List/Change Options
4: Return to Main Menu

Selection: 2								#Statistics on stat1
Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
      0.982022       0.130876       0.001309       0.001309 

2. Quantiles for each variable:

  2.5%    25%    50%    75%  97.5% 
0.7442 0.8918 0.9747 1.0672 1.2613 


------------------------------ Y.98.d vs Y.98.d ~ Poisson(θ) --------------------
Y.98.d<-menchild30nobach[,1]
mean(Y.98.d)/sd(Y.98.d)							#stat1 for Y.98.d
[1] 1.010138
hist(Y.98.stat1.pred.d)							#general look to see where the sta1 of the data falls in comparison to the 
									#stat1 of the 10000 simulated datasets; not OK

Y.98.stat1.coda.pred.d<-as.mcmc(Y.98.stat1.pred.d)			#read R output into CODA (as mcmc object) and ¡¡¡use codamenu()!!!!
codamenu()
Output Analysis ==> Plots						#gives	a) ideally, the "fuzzy catepillar"; plot((# iterations), x, type="l") 
										b) a smoothed histogram of the mcmc object (data); densityplot()

densityplot(Y.98.stat1.coda.pred.d, 								
	    xlab ="stat1 (inverse CV)"
	)

trellis.last.object(panel = function(...) {							
			panel.abline(v = 1.010138, lty = "solid", col = "black")		#add a vert line
            		panel.densityplot(...)
        }
)												#Model NOT very reasonable!!!


Output Analysis ==> Statistics						#Statistics on stat1
Iterations = 1:10000
Thinning interval = 1 
Number of chains = 1 
Sample size per chain = 10000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

          Mean             SD       Naive SE Time-series SE 
     1.1875598      0.0745453      0.0007455      0.0007455 

2. Quantiles for each variable:

 2.5%   25%   50%   75% 97.5% 
1.047 1.137 1.185 1.236 1.341 





##########################################################################################################
################ One-step Gibbs Sampler (MCMC) in BUGS ######################################################
##########################################################################################################

http://www.openbugs.net/Manuals/Scripts.html

#### reverse-DAG mapping for BUGS ########################################################
Y.99.d ~ dpois(Z1)			#w/ bachelor's degree

	==> Z1 ~ dgamma(W1, W2)

		==>W1 = 2
		==>W2 = 1 

Y.98.d ~ dpois(Z2)			#w/o bachelor's degree

	==> Z2 ~ dgamma(V1, V2)

		==>V1 = 2
		==>V2 = 1 


#### data prep2 in R #################################################################

menchild30bach.dat		
menchild30nobach.dat	##Poisson data comes in as space-separated w/ multiple lines:
			##	==> w/ "Whitespace" trying to import into R will give error: "Line 2 did not have 38 elements"
			##	==> w/ "Comma" importing will give just ONE variable (V1) with all numbers in a string; NOT a vector!!
			##	==> CAN'T use "bugsData() to create BUGS-ready file because data needs to already be in R

			##	==> use scan()
					*export
					*clean-up
			##	==> use scan()
					*use bugsData()
			##	==> use "Replace" in Leafpad to change 'space-bar' to ','
					*clean-up

menchild30bach <- as.data.frame(scan("~/Desktop/menchild30bach.dat", nlines=2))
write.table(menchild30bach,
            file = "/home/bmarron/Desktop/Y_99.d.txt", 			
            sep = ",",
	    eol = ",",
            col.names = FALSE, 
            row.names=FALSE
)


menchild30nobach <- as.data.frame(scan("~/Desktop/menchild30nobach.dat", nlines=6))
write.table(menchild30nobach,
            file = "/home/bmarron/Desktop/Y_98.d.txt", 			
            sep = ",",
	    eol = ",",
            col.names = FALSE, 
            row.names=FALSE
)



###### the BUGS model ###################################################### 
model{

##-------exploratory --------------------------------------
##---- p(pred.data) <== ∫p(pred.data|θ) x p(θ)  [prior predicitive from integration of (presumed likelihood) x (prior)] ----- ##
##---- p(θ|data) <== p(data|θ) x p(θ) [posterior from (likelihood) x (prior)] ----------------------##

for (i in 1:58){				
	Y.99.d[i] ~ dpois(Z1)
}
Z1 ~ dgamma(W1, W2)


for (i in 1:218){				
	Y.98.d[i] ~ dpois(Z2)
}
Z2 ~ dgamma(V1, V2)



##---- p(pred.data|data) <== ∫p(pred.data|θ) x p(θ|data)  [posterior predicitive from integration of (presumed likelihood) x (posterior)] ----- ##

Y.99.pred ~ dpois(Z1)				#prediction method 1 (p.194 BUGS Book);
						#pulls a single-valued dataset from the Poisson dist generated by each of the Z1~ dgamma(W1, W2);
						#simulates 10000, single-valued datasets from Y.99.pred (= posterior predictive dist); 
						#datasets saved ==>  samplesStats(), samplesDensity(), samplesCoda()  

Y.98.pred ~ dpois(Z2)




for (i in 1:58){				
	Y.99.pred.d[i] ~ dpois(Z1)		
}						#pulls a 58-valued dataset from the Poisson dist generated by each iteration of the Z1~ dgamma(W1, W2);
						#simulates 10000, 58-valued datasets from Y.99.pred;
						#datasets saved ==>  samplesStats(), samplesDensity(), samplesCoda()


for (i in 1:218){				
	Y.98.pred.d[i] ~ dpois(Z2)		
}




##----------- defining new variables --------------------------

D1 <- (Y.99.pred - Y.98.pred)			
D2 <- (Y.98.pred - Y.99.pred)			
D3 <- (Y.99.pred.d[] - Y.98.pred.d[])
D4 <- (Y.98.pred.d[] - Y.99.pred.d[])


P1 <- step(Y.99.pred - Y.98.pred)		#step(Y.99.pred - Y.98.pred)  = step(Y.99.pred - Y.98.pred >=0) = step(Y.99.pred >= Y.98.pred);
						#Asks, Is (Y.99.pred >= Y.98.pred)? If yes, step()=1; 0 o/w
P2 <- step(Y.98.pred - Y.99.pred)
P3 <- step(Y.99.pred.d[] - Y.98.pred.d[])		
P4 <- step(Y.98.pred.d[] - Y.99.pred.d[])


Y.99.stat1.d <- mean(Y.99.d[])/sd(Y.99.d[])				#collected from a single dataset; a constant
Y.99.stat1.pred.d <- mean(Y.99.pred.d[])/sd(Y.99.pred.d[])	#collect stat1 from each simulated dataset and save

Pstat1.99 <- step(Y.99.stat1.pred.d - Y.99.stat1.d)		#ideally, should be .5000 
								# (ie Y.99.stat1.d sits right in the middle of the Y.99.stat1.pred.d dist)


Y.98.stat1.d <- mean(Y.98.d[])/sd(Y.98.d[])
Y.98.stat1.pred.d <- mean(Y.98.pred.d[])/sd(Y.98.pred.d[])	
Pstat1.98 <- step(Y.98.stat1.pred.d - Y.98.stat1.d)



##--------- end model -------------------------------------------	
}




########### to datafile.txt ################################

list(
	W1=2,
	W2=1,
	V1=2,
	V2=1,

Y.99.d = c(1,0,0,1,2,2,1,5,2,0,0,0,0,0,0,1,1,1,0,0,0,1,1,2,1,3,2,0,0,3,0,0,0,2,1,0,2,1,0,0,1,3,0,1,1,0,2,0,0,2,2,1,3,0,0,0,1,1),


Y.98.d = c(2,2,1,1,2,2,1,2,1,0,2,1,1,2,0,2,2,0,2,1,0,0,3,6,1,6,4,0,3,2,0,1,0,0,0,3,0,0,0,0,0,1,0,4,2,1,0,0,1,0,3,2,5,0,1,1,2,1,2,1,2,0,0,0,
2,1,0,2,0,2,4,1,1,1,2,0,1,1,1,1,0,2,3,2,0,2,1,3,1,3,2,2,3,2,0,0,0,1,0,0,0,1,2,0,3,3,0,1,2,2,2,0,6,0,0,0,2,0,1,1,1,3,3,2,1,1,0,1,0,0,2,0,2,
0,1,0,2,0,0,2,2,4,1,2,3,2,0,0,0,1,0,0,1,5,2,1,3,2,0,2,1,1,3,0,5,0,0,2,4,3,4,0,0,0,0,0,0,2,2,0,0,2,0,0,1,1,0,2,1,3,3,2,2,0,0,2,3,2,4,3,3,4,
0,3,0,1,0,1,2,3,4,1,2,6,2,1,2,2)


)




########## to initsfile.txt ######################################

list(
	
) 






################ running the model #####################
---R prep and BUGS sequence---------------------------------------------------------------------------------
library("BRugs", lib.loc="/usr/local/lib/R/site-library")
library("coda", lib.loc="/usr/local/lib/R/site-library")

getwd()
oldwd<-getwd()	
modelSetWD("/home/bmarron/Desktop/Statistics/STAT_510/Working_Models")


THE IDEAL:
tester1<-BRugsFit(
     modelFile="model.txt",
     data="data.txt",
     ##inits="inits.txt",
     parametersToSave=c( "Z1", "Z2", "D1", "D2", "D3", "D4", "P1", "P2", "P3", "P4", 
			"Y.99.stat1.d", "Y.99.stat1.pred.d", "Pstat1.99", 
			"Y.98.stat1.d", "Y.98.stat1.pred.d", "Pstat1.98",
			"Y.99.pred", "Y.98.pred"),	
     numChains=1,
     nIter=20000)

tester1


IF PROBLEMS:
modelCheck("model.txt")
modelData("data.txt")
modelCompile(numChains=1)
modelInits("inits.txt")
modelGenInits()
set.seed(47)
modelUpdate(10000)

#### variables to monitor (exclude constants) ####
samplesSet(c("Z1", "Z2", "D1", "D2", "P1", "P2"))
summarySet(c("Z1", "Z2", "D1", "D2", "P1", "P2"))
modelUpdate(50000)

summaryStats(c("Z1", "Z2", "D1", "D2", "P1", "P2"))

------------ output -----------------------------------------------
tester1
$Stats
             mean      sd  MC_error val2.5pc median val97.5pc start sample
D1        -0.4561 1.54000 0.0109300  -4.0000 0.0000     2.000  1001  20000
D2         0.4561 1.54000 0.0109300  -2.0000 0.0000     4.000  1001  20000
P1         0.5258 0.49930 0.0033570   0.0000 1.0000     1.000  1001  20000
P2         0.7484 0.43390 0.0029480   0.0000 1.0000     1.000  1001  20000
Y.98.pred  1.3970 1.18500 0.0089440   0.0000 1.0000     4.000  1001  20000
Y.99.pred  0.9407 0.98510 0.0075580   0.0000 1.0000     3.000  1001  20000
Z1         0.9500 0.12760 0.0008847   0.7139 0.9451     1.215  1001  20000
Z2         1.4020 0.07988 0.0006253   1.2500 1.4000     1.563  1001  20000

$DIC
        Dbar  Dhat   DIC     pD
Y.98.d 704.8 703.8 705.8 0.9908
Y.99.d 154.5 153.5 155.5 0.9796
total  859.3 857.3 861.2 1.9700

--- outputs ------------------------------
mean(Y.99.pred)
[1] 0.9509
mean(Y.98.pred)
[1] 1.4032

mean(Z1.mc)
[1] 0.9492558
mean(Z2.mc)
[1] 1.402185

mean(Y.99.pred - Y.98.pred)
[1] -0.4523
mean(Y.98.pred - Y.99.pred)
[1] 0.4523


mean(Y.99.pred>Y.98.pred)		#predictive prob that pop Y.99 has more children than pop Y.98
[1] 0.2555
mean(Y.99.pred<Y.98.pred)
[1] 0.4696








LOG1_HW4.3a

cfInv_99.pred <- mean(y_99.pred[])/sd(y_99.pred[])	#a dist b/c based on multiple datasets (10000!)
cfInv_99.d <- mean(y_99.d[])/sd(y_99.d[])		#a constant b/c based on a single dataset

cfInv_98.pred <- mean(y_98.pred[])/sd(y_98.pred[])	#a dist	
cfInv_98.d <- mean(y_98.d[])/sd(y_98.d[])		#a constant

D <- (Z-V)
p_99 <- step(cfInv_99.pred-cfInv_99.d)
p_98 <- step(cfInv_98.pred-cfInv_98.d)





############ graphics ##############################  

---change param settings in R ------------
param1<-par()		#save the old settings
par(mar = rep(2, 4))			
------------------------------------------
samplesDensity("cfInv_99.pred")		#generic desnity plots of Hoff's test stat (p. 232)
samplesDensity("cfInv_98.pred")


samplesCoda("cfInv_99.pred", "Hoff99")			#RBugs output to pkg 'coda'; <stem>=Hoffs.t_99
Hoffs.t_99<-read.openbugs("Hoff99")			#read in data to 'coda' using the <stem>
attributes(Hoffs.t_99)					#see what's in it
densityplot(Hoffs.t_99, 				#'coda' plot; a trellis ('lattice') object
	    xlab ="Hoff's test stat (inverse CV)"
)				

assessfit_99 <- trellis.last.object(panel = function(...) {				#'lattice' command
			panel.abline(v = 3.83, lty = "solid", col = "black")		#add a vert line
            		panel.densityplot(...)
        }
)



samplesCoda("cfInv_98.pred", "Hoff98")			#RBugs output to pkg 'coda'
Hoffs.t_98 <-read.openbugs("Hoff98")			#read in the data to 'coda'
densityplot(Hoffs.t_98, 				#'coda' plot; a trellis ('lattice') object
	    xlab ="Hoff's test stat (inverse CV)"
)

assessfit_98 <- trellis.last.object(panel = function(...) {				#'lattice' command
			panel.abline(v = 5.61, lty = "solid", col = "black")		#add a vert line
            		panel.densityplot(...)
        }
)





-------- fyi --------------------------------------------------------------
trellis.last.object(panel = function(...) {					#lattice command
		panel.abline(h = .2, v = 7, lty = "solid", col = "black")	#add vert and horiz lines
            	panel.densityplot(...)
        }
)
------------------------------------------------------------------------------








